# 👋 データエンジニア ポートフォリオ

## 🚀 About Me

dbtを使ったデータ変換・モデリング、分析用データマート構築の専門家として、大手企業のデータ活用を支援しています。  
分析寄りのデータエンジニアとして、データ変換から分析・施策提案まで一貫したソリューションを提供します。

一貫して「データを活用して社内チームをサポートする」ことにやりがいを感じてきました。

**営業事務時代（2011-2013年）**：花苗メーカーで、前任者がExcelを扱えず目視確認のみで在庫管理していた課題を解決するため、Excel VBA・マクロを独学で習得。通販担当社員へのヒアリングから業務課題を明確化し、3つのデータソース（通販サイト在庫、社内在庫、卸元在庫）を統合する管理システムを構築。CSV取込により「添付するだけで瞬時に在庫バランスを可視化」できる仕組みを実現。システム導入により、通販商品全体で**前年比120%**、多い商品では**前年比130%**の売上を達成。営業部長を経由して社長から「正確ですごく早い」と評価され、通販事業部から「前任者ができないことをやってくれた！本当にありがとう！」と評価していただきました。この経験から「データで需要と供給のバランスを取ることで、売上に貢献できる」「一緒に働く人をサポートする喜び」を実感し、これが私の「データエンジニアリング」の原点となりました。

**旅行代理店時代（2014-2019年）** - 💡 原体験その1：「データで人の役に立つ喜び」を知った原点：Google Analyticsで閲覧数・回遊率を分析し、Webページのデザインやメールマガジンのコンテンツを改善。独自選定ツアーのメルマガで**前週比1.3倍の反応率**を複数回達成（通常は0.8〜1.1倍）。上司から「作ったメルマガの反応良いよー！」と評価していただいた経験が、私にとって最も嬉しい瞬間でした。この「データ分析→施策実行→結果確認→評価」というサイクル全体に関わることで、自分の貢献が可視化され、社内チームの成果に繋がっていることを実感できました。データ分析で1.3倍の成果を達成し、チームから感謝された経験が、「データで貢献したい」という軸の出発点となりました。SQLでこれまで整理しきれていなかった分析データを抽出し、Pythonでデータ整形・集計。様々な予測を立ててWebデザインに活用し、データドリブンなWebデザインにより、閲覧数や回遊率が如実に変化。マーケティング施策に活かす面白さを知り、「もっとデータに深く関わりたい」と思うようになりました。

**産後・業務委託時代（2022-2023年）** - 💡 原体験その2：「エンジニアリングが必要」と気づいた転機：子育てをしながら、ECサイトのマーケティング補助として、データ分析に触れ続けました。フルタイムは難しい状況でしたが、データに関わる仕事を完全に離れたくなかったのです。手入力で2時間かかっていた資料作成業務を、Excel数式・関数の活用により30分に短縮（**75%削減**）する業務効率化を実現。しかし、この期間中、私がまとめたデータがクライアント企業で全く使われていないことが判明しました。理由は、データの使い方が分からない、データ分析の方法が分からない、データよりも優先すべきことが多い、という3点でした。日本企業の多くが、市場にあふれる貴重なビッグデータを使いこなせていない現実に愕然としました。H.I.S.やサントリーフラワーズでの経験から、データを活かせば確実に効果が出ることを実感していたからこそ、このギャップが衝撃でした。この経験から、「データ分析だけでは不十分。データ基盤（エンジニアリング）がなければ、分析も活きない。だから自分自身がエンジニアになる必要がある」と気づき、データエンジニアを目指す決意を固めました。

**データエンジニア時代（2024年1月～現在）**：データエンジニアとして1年11ヶ月、大手企業向けの分析用データマート実装に従事。アパレル4ブランドのマーケティングタグ管理基盤の統合移行（2,724項目のテスト実施、タグ発火率99.8%達成）、BIデータマート実装（分析リードタイム96%削減）、SFCC Products API連携による大規模商品データ取込基盤構築（外部ベンダーへの技術質問対応を主体で担当）、SFCC Customer API連携のリトライ処理実装、モバイルアプリデータリカバリー対応、大手不動産企業空港事業部向けIPOCAレポートSQL作成・移動データ異常値補正、backup_data WFのパラレル実行化対応など、技術的なスキルを習得しました。

しかし、データ整備だけでなく、「データを分析し、施策を提案し、チームの成果に貢献し、評価していただく」という一連のプロセスに関わりたいと強く思うようになり、転職を決意しました。また、現職では全体像（なぜこのデータが必要か、誰がどう使うか、どんな成果を目指すか）が見えず、最適な提案ができないことにも課題を感じています。

次のキャリアでは、BtoB SaaS企業で社内チームを支援する分析用データマートを構築し、dbtを使ったデータ変換・モデリングのスキルを活かしながら、企業向けサービスのデータ分析・施策提案まで一貫して関わり、全体像が見える環境で社内チームに貢献できる仕事を実現したいと考えています。

## 🔧 なぜ技術を磨くエンジニアの道を選ぶのか

### データ基盤という「仕組み」で、チーム全体を支えたい

私がエンジニアとして技術を磨くことにこだわる理由は、**「仕組みによる持続的な支援」**を実現したいからです。

旅行会社時代、メールマガジン施策で前週比1.3倍の反応率を達成した経験から、「データ分析→施策実行→結果確認→チームへの貢献」というサイクルに大きなやりがいを感じました。しかし、その後のマーケティング補助の仕事で、私が整えたデータがクライアント企業で全く使われていない現実に直面しました。理由は、データの使い方が分からない、分析方法が分からない、優先順位が低い、というものでした。

この経験から、**「分析だけでは不十分。データ基盤（エンジニアリング）がなければ、分析も活きない」**と気づきました。

現職では、データエンジニアとして1年11ヶ月、この信念を実践してきました：

- **分析リードタイムを2日から30分に短縮（96%削減）**：マーケティングチームが毎日データで意思決定できる環境を構築
- **タグ発火率を95%から99.8%に改善**：データ品質を根本的に向上させ、チーム全体の分析精度を向上
- **月40時間の作業時間削減、完全自動化を実現**：手動作業をゼロにし、チームがより価値の高い業務に集中できる環境を提供

個別の依頼に都度対応するのではなく、データパイプラインという「仕組み」を構築することで、チーム全体が自律的にデータを活用できる環境を作る。これが、私がエンジニアとして技術を磨き続ける理由です。

次のキャリアでは、dbtやBIツールを使った分析寄りのデータエンジニアリングを通じて、社内チームの成果を最大化し続けたいと考えています。

## 💫 キャリアストーリーと価値観

### 一貫したテーマ：「データを活用して社内チームをサポートする」

私のキャリアは、一見バラバラに見えるかもしれませんが、すべて「データを通じて一緒に働く人をサポートし、評価していただく」という一貫した価値観で繋がっています。

**営業事務時代（2011-2013年）**
- 花苗メーカーで、前任者がExcelを扱えず目視確認のみで在庫管理していた課題を解決
- Excel VBA・マクロを独学で習得し、3つのデータソース（通販サイト在庫、社内在庫、卸元在庫）を統合する管理システムを構築
- CSV取込により「添付するだけで瞬時に在庫バランスを可視化」できる仕組みを実現
- システム導入により、通販商品全体で前年比120%、多い商品では前年比130%の売上を達成
- 営業部長を経由して社長から評価され、通販事業部から「前任者ができなかったことをやってくれた！本当にありがとう！」と評価していただく
- 「データで需要と供給のバランスを取ることで、売上に貢献できる」「一緒に働く人をサポートする喜び」を実感
- **これが私の「データエンジニアリング」の原点**：複数データソース統合、業務理解に基づく設計、ビジネス価値の創出

**旅行代理店時代（2014-2019年）** - 💡 原体験その1：「データで人の役に立つ喜び」を知った原点
- Google Analyticsで閲覧数・回遊率を分析し、Webページのデザインやメールマガジンのコンテンツを改善
- 独自選定ツアーのメルマガで前週比1.3倍の反応率を複数回達成（通常は0.8〜1.1倍）
- 上司から「作ったメルマガの反応良いよー！」と評価していただいた経験が、私にとって最も嬉しい瞬間でした
- この「データ分析→施策実行→結果確認→評価」というサイクル全体に関わることで、自分の貢献が可視化され、社内チームの成果に繋がっていることを実感
- **データ分析で1.3倍の成果を達成し、チームから感謝された経験が、「データで貢献したい」という軸の出発点となった**
- SQLでこれまで整理しきれていなかった分析データを抽出
- Pythonでデータ整形・集計し、予測を立ててWebデザインに活用
- データドリブンなWebデザインにより、閲覧数や回遊率が如実に変化
- 「全体像（なぜこの施策が必要か、誰のためか、どんな結果を目指すか）が見えるから、最適な提案ができる」ことを実感

**産後・業務委託時代（2022-2023年）** - 💡 原体験その2：「エンジニアリングが必要」と気づいた転機
- 子育てをしながら、ECサイトのマーケティング補助として、データ分析に触れ続けました
- フルタイムは難しい状況でしたが、データに関わる仕事を完全に離れたくなかった
- 検索連動型広告管理のデータ整理・集計・分析、売上管理のデータ整理・集計・分析、Google Analytics・キーワードプランナーを活用したSEO対策資料作成
- 手入力で2時間かかっていた資料作成業務を、Excel数式・関数の活用により30分に短縮（75%削減）
- **重要な気づき**：私がまとめたデータがクライアント企業で全く使われていないことが判明
  - 理由：データの使い方が分からない、データ分析の方法が分からない、データよりも優先すべきことが多い
  - 日本企業の多くが、市場にあふれる貴重なビッグデータを使いこなせていない現実に愕然とした
  - H.I.S.やサントリーフラワーズでの経験から、データを活かせば確実に効果が出ることを実感していたからこそ、このギャップが衝撃だった
- **決意**：「データ分析だけでは不十分。データ基盤（エンジニアリング）がなければ、分析も活きない。だから自分自身がエンジニアになる必要がある」と気づき、データエンジニアを目指す決意を固めた

**データエンジニア時代（2024年1月～現在）**
- データエンジニアとして1年11ヶ月、大手企業向けの分析用データマート実装に従事
- アパレル4ブランドのマーケティングタグ管理基盤の統合移行（2,724項目のテスト実施、タグ発火率99.8%達成）
- BIデータマート実装（分析リードタイム96%削減）など、技術的なスキルを習得
- しかし、データ整備だけでなく、「データを分析し、施策を提案し、チームの成果に貢献し、評価していただく」という一連のプロセスに関わりたいと強く思うようになる

### 転職を決意した理由

データエンジニアとして技術的には成長していますが、3つの大きな課題を感じています：

1. **全体像が見えない**
   - **何が見えないのか**：Why（なぜこのデータ基盤が必要か）、Who（誰がどう使うか）、What（どんな成果を目指すか）
   - **何が問題なのか**：全体像が見えないと、最適な提案ができない。「このデータ構造の方が、後で分析しやすいのでは？」と提案したくても、誰がどう使うか分からないので提案できない。全体像が見えないことで、齟齬が生まれて無駄な作業が増え、会社にとって不利益
   - **大手旅行代理店時代との違い**：大手旅行代理店では、メルマガの目的（ツアーの予約数増加）、対象（過去の顧客）、成果指標（誘客率）がすべて明確だった。だから、データを見て「こういうツアーを紹介すべき」と施策を考えられた

2. **貢献実感がない**
   - **何が見えないのか**：このデータが、どう分析されているのか、どんな施策に繋がっているのか、どんな成果が出ているのか、マーケティングチームや営業チームが、どう活用しているのか
   - **何が問題なのか**：データを整備しても、それがどう役立っているか分からない。「データを整備する」だけで終わってしまい、「データを使って施策を考え、結果を確認する」という一連のサイクルに関われない。貢献が可視化されないため、成長実感が得られない
   - **大手旅行代理店時代との違い**：大手旅行代理店では、「データ分析→施策実行→結果が数値で見える→上司から評価していただく」という完結したサイクルがあった。自分の貢献が可視化され、上司から「私のメルマガの反応良いよー！」と評価していただいた

3. **フィードバックが得られる機会がない**
   - **現職の状況**：ミスの指摘や注意の言葉は来るが、ポジティブなフィードバックがほとんどない。「データ基盤を整備した」という技術的な貢献はしているはずだが、誰からも評価していただけない
   - **自分にとって重要な理由**：自己肯定感が低いため、評価していただくことで「自分の存在価値」を実感できる。大手旅行代理店時代の「私のおかげで反応良いよー！」という言葉が、最も嬉しい瞬間だった。一緒に働く人から評価していただくことが、仕事のモチベーションになる
   - **大手旅行代理店時代との違い**：大手旅行代理店では、上司や同僚から直接「ありがとう」と言われる環境だった。社内チームへの直接貢献だったからこそ、評価していただく機会があった

### 理想の職種：「データエンジニア + アナリスト（マーケティング支援）」

- **データに触れる時間**：80-100%
- **仕事内容**：dbtを使ったデータ変換・モデリング + データ分析 + 施策提案
- **貢献先**：社内チーム（マーケティング・営業等）への直接貢献
- **成果確認**：結果を数値で確認し、フィードバックが得られる環境

大手旅行代理店時代の「データ分析→施策実行→結果確認→評価」というサイクルをもう一度経験したい。データエンジニアリングのスキルと、以前の分析経験を組み合わせることで、社内チームに対してより大きな価値を提供できると考えています。

### 💡 新たな気づき：本当にやりたいこと

データエンジニアとして働く中で気づいたこと：

- **「データに触れるだけでは足りない」**：データ変換・モデリングだけでは、満たされない。大手旅行代理店時代の「データ分析→施策→結果確認→評価」というサイクル全体に関わりたい
- **「全体像が見える環境で働きたい」**：Why/Who/Whatが分からない状態で作業をするのは、自分にとって苦痛。全体像が見えて初めて、最適な提案ができる
- **「社内チームへの直接貢献がしたい」**：エンドユーザーの反応よりも、一緒に働く人から「私のおかげで」と評価していただきたい。マーケティングチームや営業チームが「データのおかげで分析が楽になった」「施策が成功した」と喜んでくれる環境
- **「データエンジニアリング + 分析・施策提案 = 理想の仕事」**：dbtを使ったデータ変換・モデリングのスキル（強み）を活かしながら、データ分析・施策提案まで一貫して関わる。これが、自分にとって最も価値を発揮できる働き方

### 💼 専門領域
- **dbtを使ったデータ変換・モデリング**: BigQuery, Treasure Data, GCPを活用した分析用データマート構築
- **ETL/ELT開発**: 複雑なデータ変換処理とパイプライン自動化
- **マーケティング分析**: GA4/GTM実装、タグ管理基盤移行、大規模統合移行プロジェクト

## 🛠️ Technical Skills

### 【データエンジニアリング】
- **SQL**（6年以上）：CTE、Window関数、複雑なJOIN、700行超の大規模クエリ開発
- **Python**（3年以上）：pandas、numpy、seaborn、selenium、requests、cryptography
- **JavaScript**：UDF開発、カスタムアルゴリズム実装、GTMカスタムHTML開発
- **クラウド**：GCP（BigQuery、Cloud Functions、Secret Manager）
- **データ基盤**：Treasure Data、Digdag
- **ETLパイプライン開発**：SFTP連携、API連携、データクレンジング

### 【マーケティング支援・データ分析】
- **マーケティングツール**：GTM、GA4、Tealium、Braze、LINE STAFF START、Silver Egg、Repro
- **GA4イベントトラッキング設計・実装**（11種類のEコマースイベント）
- **大規模タグ管理基盤の統合移行**（2,400項目、4ブランド並行、2,724項目の網羅的テスト実施）
- **メールマーケティング施策の企画・実行・効果測定**（前週比1.3倍の反応率達成）
- **Google Analytics、キーワードプランナーを活用したデータ分析・SEO対策**

### 【業務効率化】
- **Excel VBA、数式・関数を活用した業務自動化**（作業時間75%削減実績）
- **問題発見力・課題解決力**
- **AI開発支援ツール**（Claude、Cursor、GitHub Copilot）を活用した開発効率化

### データ基盤 & クラウド
![BigQuery](https://img.shields.io/badge/BigQuery-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)
![Treasure Data](https://img.shields.io/badge/Treasure_Data-FF6B6B?style=for-the-badge&logoColor=white)
![GCP](https://img.shields.io/badge/Google_Cloud-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)

### プログラミング言語
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![SQL](https://img.shields.io/badge/SQL-4479A1?style=for-the-badge&logo=postgresql&logoColor=white)
![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black)

### ワークフロー & オーケストレーション
![Digdag](https://img.shields.io/badge/Digdag-2088FF?style=for-the-badge&logoColor=white)
![Cloud Functions](https://img.shields.io/badge/Cloud_Functions-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)

### 分析 & 可視化
![Tableau](https://img.shields.io/badge/Tableau-E97627?style=for-the-badge&logo=tableau&logoColor=white)
![GA4](https://img.shields.io/badge/Google_Analytics_4-E37400?style=for-the-badge&logo=google-analytics&logoColor=white)
![GTM](https://img.shields.io/badge/Google_Tag_Manager-246FDB?style=for-the-badge&logo=google-tag-manager&logoColor=white)

### AI開発支援ツール
![Claude](https://img.shields.io/badge/Claude-FF6B6B?style=for-the-badge&logoColor=white)
![Cursor](https://img.shields.io/badge/Cursor-4285F4?style=for-the-badge&logoColor=white)
![GitHub Copilot](https://img.shields.io/badge/GitHub_Copilot-181717?style=for-the-badge&logo=github&logoColor=white)

## 📊 実績データ

<table>
<tr>
<td align="center">
<h3>5件以上</h3>
<p>大規模プロジェクト</p>
</td>
<td align="center">
<h3>96%</h3>
<p>処理時間削減</p>
</td>
<td align="center">
<h3>99.9%</h3>
<p>システム稼働率</p>
</td>
<td align="center">
<h3>500万+</h3>
<p>日次処理レコード</p>
</td>
</tr>
</table>

## 🏆 主要プロジェクト

### 1. 👟 スポーツアパレル企業 SFCC API連携による大規模商品データ取込基盤構築
**役割**: データエンジニア（実装担当）  
**期間**: 2025年8月～（現在継続中）  
**チーム規模**: 3名

- Salesforce Commerce Cloud SCAPI Products APIを用いた商品マスタデータ自動取込システムを開発。1日あたり最大2万件の更新が発生する大規模データを、API制限やエラーに対応しながら確実に取得する仕組みを実装
- **課題**: 
  - 1日あたり最大2万件の商品更新が発生し、特定の秒に数千〜数万件の更新が集中する可能性
  - API制限（20件/リクエスト、レスポンスサイズ10MB上限、1,500件/期間）への対応
  - Response Entity Too Largeエラー（500エラー）の発生
  - 累計10,000件超のメモリ効率的な処理が必要
- **技術実装**: 
  - OAuth 2.0認証（Client Credentials Flow）とトークン自動更新機能による長時間処理の安定化
  - バイナリサーチによる適応的期間分割アルゴリズムで、データ量に応じて日単位・時間単位を自動切り替え（1,500件超で自動分割）
  - Response Entity Too Largeエラー（10MB制限）発生時の自動期間再分割処理（最大5回リトライ）
  - 10万件単位のバッチ処理とメモリクリアによる効率的な大容量データ処理
  - 150項目以上のカラムマッピング辞書による自動変換（キャメルケース→スネークケース）
  - 5段階CTE処理による重複排除と完全なデータ正規化
  - **外部ベンダーへの技術質問文書の作成（主体で担当）**: 大量データ取得における技術的課題を明確化し、質問事項を項目分け（大量データ取得、API制限、推奨実装方法など）
  - **外部ベンダーからの質問に対する回答文書の作成（主体で担当）**: 実装コードから確認できる内容を明確に記載し、やり取りの文章も主体で作成・管理
- **成果**: 
  - Response Entity Too Largeエラーを自動処理し、安定した大容量データ取込を実現
  - バイナリサーチによる期間最適化でAPI呼び出し回数を最小化
  - 10万件バッチ処理でメモリ効率的な大量データ処理を実現
  - 150項目以上の複雑なカラムマッピングを正確に実装
  - 外部ベンダー（外部ベンダー）との技術的なコミュニケーションを主体で担当
- **技術**: Treasure Data, Digdag, Python (requests, pytd, pandas, json, datetime, time, os, base64), Salesforce Commerce Cloud SCAPI, OAuth 2.0, Presto SQL

### 2. ✈️ 大手不動産企業 空港事業部 GPS位置情報異常値検出・補正システムの開発
**役割**: データエンジニア（実装担当）  
**期間**: 現在継続中  
**チーム規模**: 2名

- 国内旅行レンタルデバイスから収集されるGPS位置情報の異常値を自動検出・補正するシステムを開発
- **課題**: 時速1000km超の非現実的な移動、V字型異常パターン、座標の急激な変動により、正確な旅行経路分析が困難
- **技術実装**: 
  - JavaScript UDFによるHaversine公式を用いた球面距離計算アルゴリズム
  - 時速1000km超の非現実的な移動検出、V字型異常パターン検出、適応的傾向分析による座標補正
  - Window関数（LAG/LEAD/FIRST_VALUE/LAST_VALUE）を活用した時系列データ処理と日付間の連続性担保
  - 450行超のSQLクエリを10段階以上のCTE処理に分解し、ARRAY_AGG/UNNESTによる配列データ処理
- **成果**: 
  - GPS位置情報の異常値を自動検出・補正し、正確な旅行経路データの生成を実現
  - 450行超の大規模クエリによる複雑なデータ変換パイプラインを構築
  - クライアント要望への即日対応により、継続的なロジック改善を実現
  - 旅行経路分析の精度向上により、ビジネス判断の質を改善
- **技術**: BigQuery, Standard SQL, JavaScript UDF, Haversine距離計算, CTE, Window関数

### 3. 📚 学習管理システムとBigQueryを連携するETLパイプライン構築
**役割**: データエンジニア

- 企業向け学習管理システムの利用データを分析基盤に集約するため、日次でデータを自動取得・変換・格納するETLパイプラインをGCP上に構築
- **技術実装**: 
  - Cloud FunctionsでのPython ETLパイプライン開発（約6種類のデータ処理）
  - AES-256暗号化による個人情報保護（暗号化キーはSecret Managerで管理）
  - Cloud Loggingと連携した2段階アラートシステム（WARNING/ERRORレベル）
  - GET/POST両方に対応した汎用的な取得関数を実装し、テーブル固有の条件をパラメータ化
- **成果**: 
  - 日次バッチの安定稼働率99.9%を達成
  - エラー発生から対応までの時間を平均30分以内に短縮
  - 手動作業ゼロの完全自動化を実現
  - データ分析チームが即座に活用できる環境を実現
- **技術**: BigQuery, Cloud Functions, Python (cryptography, requests, google-cloud-bigquery, google-cloud-secret-manager, google-cloud-logging), AES-256暗号化, OAuth 2.0

### 4. 👗 アパレル4ブランド マーケティングタグ管理基盤の大規模統合移行プロジェクト
**役割**: データエンジニア（実装・検証担当）  
**期間**: 2024年6月〜2025年1月  
**チーム規模**: 3名

- アパレル4ブランドのタグ管理システム（Tealium、Braze、LINE STAFF START、Silver Egg、Repro等）をGoogle Tag Manager（GTM）に統合移行
- **プロジェクト規模**: 
  - 総作業項目数：2,400以上（変数：886個、トリガー：393個、カスタムHTMLタグ：646個、タグ構成：548個）
  - GA4 Eコマースイベント：11種類（view_item、add_to_cart、purchase等）
  - 総テスト項目数：2,724項目（変数テスト1,068項目、拡張機能テスト689項目、GA4タグテスト448項目、その他タグテスト519項目）
- **技術実装**: 
  - Tealium→GTMのマッピングシート作成（4ブランド中2.5ブランド分を担当）
  - カスタムHTML/JavaScriptタグ開発（646個のロジック移植）
  - GA4イベント設計・実装（11種類：view_item_list、view_item、sign_up、search、begin_checkout、add_payment_info、purchase、refund、remove_from_cart、add_to_cart、view_cart）
  - 複雑なJavaScriptロジックの移植（条件分岐、配列処理、データレイヤー操作）
  - 網羅的テスト実施：2,724項目
- **成果**: 
  - マーケティングチームのタグ管理業務の統合・効率化を実現
  - タグ発火率を95%から99.8%に改善（2,724項目の網羅的テストにより全タグの正常動作を確認）
  - 4ブランドの並行移行を完遂
  - マーケティングチームがデータ活用しやすい環境を整備
- **技術**: GTM, GA4, JavaScript, dataLayer, Tealium, Braze, LINE STAFF START, Silver Egg, Repro

### 5. 🏠 不動産投資会社向けBigQueryデータ分析基盤構築
**役割**: データエンジニア（設計・実装担当）  
**期間**: 2024年2月（短期集中開発）  
**チーム規模**: 3名

- 物件評価・税務計算の自動化システムをBigQueryで構築
- **課題**: 
  - Excel管理されていた物件情報（購入検討物件100件以上）の分析が属人化
  - 固定資産税・都市計画税の計算が手動で、ミスが頻発（10%のエラー率）
  - 経営層の投資判断に3日以上かかっていた
- **技術実装**: 
  - 90カラム以上の大規模テーブル設計（物件基本情報、税務情報、賃貸情報）
  - 700行超のSQLクエリ開発（5段階CTE）
  - 複雑な税務計算ロジックの実装（想定/実績の2パターン対応）
  - 文字化けデータ（Shift-JIS/UTF-8混在）、日付フォーマット不統一の完全クレンジング
- **成果**: 
  - 投資判断のスピードを3日から即日に短縮
  - 税務計算の正確性が100%に向上（手動計算時は10%のエラー率）
  - 月40時間かかっていた物件評価作業を即座に実行可能に
- **技術**: BigQuery, StandardSQL, Google Sheets API, CTE

### 6. 👟 スポーツアパレル企業 BIデータマート実装・大規模データ集計基盤開発
**役割**: データエンジニア（実装担当）  
**期間**: 2024年6月〜（現在継続中）  
**チーム規模**: 5名（エンジニア3名、アナリスト2名）

- 年月週日別・チャネル別・会員属性別の多次元集計データマートを実装
- **課題**: 経営層が必要とする売上分析が、複数のデータソースを手動で結合して行われており、分析に2日以上かかっていた
- **技術実装**: 
  - 300行超の大規模SQLクエリ開発（12パターンのUNION ALLで多次元集計を実現）
  - 5段階CTEによる段階的データ加工とWindow関数（DENSE_RANK、ROW_NUMBER）による購買順序・顧客判定
  - 2018年以降の全取引データ（約500万レコード）を10分以内で集計
  - Digdagによるワークフロー制御・ジョブスケジューリング構築
- **成果**: 
  - 分析リードタイムを2日から30分に短縮（96%削減）
  - 経営層が迅速に意思決定できる環境を実現
- **技術**: Treasure Data, Presto SQL, Tableau, CTEs, Window関数, Digdag

### 7. 🎮 大手玩具メーカー マーケティングデータ連携基盤の構築
**役割**: データエンジニア（実装担当）  
**期間**: 2024年4月〜（現在継続中）  
**チーム規模**: 3名

- Salesforce Marketing Cloud（マーケティングオートメーション）から分析基盤（Treasure Data）へのSFTP経由自動データ連携パイプラインを構築
- **課題**: 月40時間の作業時間が手動データ連携に費やされていた
- **技術実装**: 
  - YAMLベースの設定ファイルとDigdagワークフローによる日次バッチ自動化（約10万レコード/日）
  - データ型不整合問題（Long型に空文字混入）を2段階処理で解決（L1層String型→L2層Long型変換）
  - ログ分析により2時間以内に問題特定し、3時間以内に本番環境修正完了
- **成果**: 
  - 月40時間の作業時間削減、手動作業ゼロの完全自動化を実現
  - データ品質問題を3時間以内に解決し、マーケティングチームのビジネス影響を最小化
  - 技術文書（テーブル定義書、ワークフロー仕様書、テスト項目書）を整備し、保守性を向上
- **技術**: Treasure Data, Digdag, SFTP, Presto SQL, YAML

### 8. 🔄 スポーツアパレル企業 SFCC Customer APIリトライ処理実装
**役割**: データエンジニア（実装担当）  
**期間**: 2025年11月4日～11月13日  
**チーム規模**: 3名

- Salesforce Commerce Cloud（SFCC）からTreasure Dataへの顧客データ取込処理において、一時的なネットワークエラーやAPIサーバー側の問題により処理が失敗するケースが発生していた課題を解決
- **課題**: 
  - タイムアウトエラー、接続エラー、5xxエラー、4xxエラー、401認証エラー、OAuthトークン取得エラー、推定件数取得失敗エラー、0件エラーなど、複数のエラーパターンが発生
- **技術実装**: 
  - Digdagワークフローにリトライ設定を追加（リトライ回数上限：3回、リトライ間隔：120秒）
  - Pythonスクリプトで各エラーパターンに対する例外処理を実装
  - 検証環境での全エラーパターンテスト実施（0件エラー、4xx/5xxエラー、401認証エラー等）
  - 本番環境での実運用動作確認（500エラー発生時の自動リトライ成功を確認）
- **成果**: 
  - 一時的なエラーからの自動復旧機能を実現
  - 運用負荷の軽減（エラー発生時の手動リラン作業が不要に）
  - データ連携の確実性向上（401認証エラー、0件エラーもリトライ対象）
- **技術**: Treasure Data, Digdag, Python (requests, pytd), OAuth 2.0, Presto SQL

### 9. 📱 スポーツアパレル企業 モバイルアプリ行動データ取込エラー リカバリー対応
**役割**: データエンジニア（リカバリー実装担当）  
**期間**: 2025年11月7日～11月10日  
**チーム規模**: 2名

- モバイルアプリ行動データ連携基盤において、データ提供元がS3バケットへのデータ配置を行わなかったため、行動データの取込が0件となるインシデントが発生。データ復旧後のリカバリー対応を実施
- **課題**: 
  - 2025年11月7日のスケジュール実行において、行動データ（behavioral_data）の取込が0件となるインシデント
  - 11月10日にデータ提供元側でS3配置が復旧したため、欠損データのリカバリー対応が必要
- **技術実装**: 
  - Embulk設定ファイル（behavioral_data.yml）の修正（リカバリー対象日の固定値設定）
  - timeカラムの固定値設定（UNIXタイムスタンプ：1762441200 = 2025-11-06 00:00:00 JST）
  - ワークフロー手動実行によるリカバリー処理実施
  - データ整合性確認クエリの実行
- **成果**: 
  - 11/6分の行動データ（787,303レコード）を正常に取込
  - データの連続性を維持
  - 実作業時間1時間でリカバリー完了
- **技術**: Treasure Data, Digdag, Embulk, Presto SQL, Hive, AWS S3

## 💡 技術的な強み

### 1. 大規模データ処理
- 500万レコード/日の高速処理実装
- 複雑なSQL（300行超、700行超、12パターンのUNION ALL）の最適化
- メモリ効率を考慮したパフォーマンスチューニング
- 10万件単位のバッチ処理とメモリクリアによる効率的な大容量データ処理

### 2. データ品質管理
- データ型不整合問題の迅速な解決（L1/L2層での段階的変換）
- 文字化け・フォーマット不統一への対応
- 包括的なテスト戦略（単体・結合・回帰テスト）
- 2,724項目の網羅的テストによる品質保証

### 3. チーム支援・教育
- ドキュメント整備による属人化防止
- AI開発支援ツール（Claude、Cursor、GitHub Copilot）を活用した開発効率化

### 4. マーケティングデータ基盤
- 大規模タグ管理基盤の統合移行（2,400項目、4ブランド並行）
- GA4イベントトラッキング設計・実装（11種類のEコマースイベント）
- 複雑なJavaScriptロジックの移植（条件分岐、配列処理、データレイヤー操作）

### 5. 地理空間データ分析
- JavaScript UDFによるHaversine公式を用いた球面距離計算
- 異常値検出・補正アルゴリズムの開発
- 時系列データ処理と日付間の連続性担保

## 📈 ビジネスインパクト

- **業務効率化**: 手動作業を月40時間削減、手動作業ゼロの完全自動化を実現
- **意思決定の高速化**: 分析時間を2日から30分に短縮（96%削減）、投資判断を3日から即日に短縮
- **品質向上**: エラー率を10%から0.2%以下に改善、タグ発火率を95%から99.8%に改善、税務計算の正確性を100%に向上
- **コスト削減**: 自動化による人的リソースの最適化
- **データ活用基盤の整備**: マーケティングチームがデータを活用しやすい環境を構築

## 🎯 得意とする課題解決

✅ レガシーシステムのモダナイゼーション  
✅ 分析用データマート構築  
✅ 複雑なビジネスロジックのSQL実装  
✅ マルチソースデータの統合と正規化  
✅ スケーラブルなデータアーキテクチャ設計  
✅ **データ分析から施策提案まで一貫した支援**（大手旅行代理店時代のメルマガ成功体験：前週比1.3倍の反応率達成）  
✅ **社内チームへの直接貢献**（営業事務時代：前年比120%の売上貢献、広告運用コンサルティング会社：作業時間75%削減）

## 🌟 転職先に求める環境

### Must条件
- ✅ フルリモート・フルフレックス
- ✅ 全体像が見える環境（Why/Who/Whatが明確）
- ✅ 社内チーム（マーケ・営業等）への直接貢献
- ✅ 結果が数値で確認できる
- ✅ ポジティブなフィードバック文化（評価していただける環境）
- ✅ データに触れる時間が80%以上

### Want条件
- ✅ 分析・施策提案の機会がある
- ✅ データエンジニアリング + アナリスト業務のハイブリッド型
- ✅ 提案できる余地がある  

## 📫 Contact

- 🌐 [ポートフォリオサイト](https://sato1046.github.io)
- 💼 副業・業務委託のご相談はお気軽にご連絡ください

## 🔄 継続的な学習

常に最新技術をキャッチアップし、以下の分野でスキルを拡張しています：
- クラウドネイティブアーキテクチャ
- リアルタイムストリーミング処理
- MLOps/データサイエンス基盤
- データガバナンス・セキュリティ

---

### 🌟 お仕事のご依頼について

dbtを使ったデータ変換・モデリング、分析用データマート構築などのプロジェクトでお困りの際は、ぜひご相談ください。  
短期集中型のプロジェクトから長期的な支援まで、柔軟に対応いたします。

**対応可能な業務:**
- dbtを使ったデータ変換・モデリング
- 分析用データマート構築
- ETL/ELTパイプライン開発
- データ品質改善
- 技術支援・コンサルティング

---

*"データを価値に変える" - 複雑なデータ課題を、実用的なソリューションで解決します。*
